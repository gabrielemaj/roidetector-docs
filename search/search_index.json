{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"ROI Detector Documentation","text":"<p>Machine learning application for automated Region of Interest detection on fluorescence microscopy images of lab mice.</p>"},{"location":"#what-is-roi-detector","title":"What is ROI Detector?","text":"<p>ROI Detector is a web-based tool that uses deep learning to automatically identify and localize anatomical regions on mouse images. The application combines computer vision, interactive editing, and statistical analysis to streamline the ROI annotation workflow for researchers.</p> <p>Key capabilities:</p> <ul> <li> <p>Automated prediction of 7 anatomical ROIs per mouse</p> </li> <li> <p>Dual-input models leveraging both photograph and fluorescence data</p> </li> <li> <p>Interactive editing with drag-and-drop interface</p> </li> <li> <p>Statistical analysis including luminance profiling and significance testing</p> </li> <li> <p>Export compatibility with existing imaging system formats</p> </li> </ul>"},{"location":"#features","title":"Features","text":""},{"location":"#automated-roi-prediction","title":"Automated ROI Prediction","text":"<p>Pre-trained ResNet50-based models predict 7 anatomical regions:</p> <ol> <li> <p>Head</p> </li> <li> <p>Upper body</p> </li> <li> <p>Mid body</p> </li> <li> <p>Lower body</p> </li> <li> <p>Genital area</p> </li> <li> <p>Foot</p> </li> <li> <p>Tail</p> </li> </ol>"},{"location":"#dual-input-support","title":"Dual-Input Support","text":"<p>Choose between:</p> <ul> <li> <p>Single-input model - Photograph only</p> </li> <li> <p>Dual-input model - Photograph + fluorescence for enhanced accuracy</p> </li> </ul>"},{"location":"#interactive-editing","title":"Interactive Editing","text":"<ul> <li> <p>Drag-and-drop crop definition</p> </li> <li> <p>Move, rotate, and adjust predicted ROIs</p> </li> <li> <p>Keyboard shortcuts for precise control</p> </li> <li> <p>Constraint-based editing (ROIs stay within crop bounds)</p> </li> </ul>"},{"location":"#fluorescence-analysis","title":"Fluorescence Analysis","text":"<ul> <li> <p>Luminance threshold visualization</p> </li> <li> <p>Intensity profile charting</p> </li> <li> <p>Paired t-test for signal significance</p> </li> <li> <p>User evaluation logging</p> </li> </ul>"},{"location":"#exportimport","title":"Export/Import","text":"<ul> <li> <p>Export to <code>AnalyzedClickInfo.txt</code> format (imaging system compatible)</p> </li> <li> <p>Import existing ROIs for editing</p> </li> <li> <p>Optional metadata file merging</p> </li> </ul>"},{"location":"developer-guide/api-reference/","title":"API Reference","text":"<p>Swagger docs: <code>https://ai-roi-detector.biodata.di.unimi.it/api/docs</code> Main technologies: Python and FastAPI</p> <p>Run locally - requires Python 3.11: <pre><code>conda activate ottobrini\ncd single_output\nuvicorn api.server:app --port 4002 --reload --host 0.0.0.0\n</code></pre></p>"},{"location":"developer-guide/api-reference/#api-overview","title":"API Overview","text":"<p>The ROI Detector API provides RESTful endpoints for image processing, model inference, and geometry utilities. All endpoints are prefixed with <code>/api</code> and documented automatically via FastAPI's Swagger UI.</p> <p>Core functionality:</p> <ul> <li> <p>Image management - Upload, preview, and cache TIFF/PNG/JPEG images</p> </li> <li> <p>Model inference - Run trained ResNet50 models on user-defined crops</p> </li> <li> <p>Geometry utilities - Calculate rotated rectangle corners and point-polygon distances</p> </li> <li> <p>Data logging - Save user evaluations to CSV for analysis</p> </li> </ul> <p>For complete endpoint documentation, parameters, and response schemas, visit the Swagger docs at <code>/api/docs</code>.</p>"},{"location":"developer-guide/api-reference/#add-a-new-endpoint","title":"Add a New Endpoint","text":"<p>Adding a new endpoint is as simple as adding a function to <code>api/server.py</code>. HTTP methods, path parameters, and query parameters are automatically inferred from the function signature. For more details, refer to FastAPI's path operation documentation.</p> <p>Example: <pre><code>@api.get(\"/example\")\ndef example_endpoint(param: str):\n    return {\"message\": f\"Hello {param}\"}\n</code></pre></p>"},{"location":"developer-guide/api-reference/#work-with-tensorflow-models","title":"Work with TensorFlow Models","text":"<p>The API loads trained Keras models on-demand and caches them in memory: <pre><code>MODEL_CACHE = {}\n\ndef get_model(model_name: str):\n    if model_name not in MODEL_CACHE:\n        model_path = get_model_path(model_name)\n        MODEL_CACHE[model_name] = tf.keras.models.load_model(\n            model_path, compile=False\n        )\n    return MODEL_CACHE[model_name]\n</code></pre></p> <p>To add a new model: 1. Train and save to <code>outputs/run_YYYYMMDD_HHMMSS/fine_tune_last_n/model_finetuned_last_n.keras</code> 2. Add entry to <code>AVAILABLE_MODELS</code> dict in <code>api/server.py</code> <pre><code>AVAILABLE_MODELS = {\n    \"run_20251208_105627\": {\"dual_input\": False},\n    \"run_20251219_171038\": {\"dual_input\": True},\n    \"run_20250115_120000\": {\"dual_input\": False}  # New model\n}\n</code></pre></p>"},{"location":"developer-guide/api-reference/#work-with-image-processing","title":"Work with Image Processing","text":"<p>The API uses Pillow (PIL) for image manipulation. Key functions:</p>"},{"location":"developer-guide/api-reference/#auto_level_to_8bitim-use_percentilefalse","title":"<code>auto_level_to_8bit(im, use_percentile=False)</code>","text":"<p>Normalizes 16-bit TIFF images to 8-bit range: - <code>use_percentile=False</code> - Linear min-max scaling - <code>use_percentile=True</code> - Percentile-based (1st-99.5th) for fluorescence images</p>"},{"location":"developer-guide/api-reference/#make_model_canvas_and_metabase_img-xc-yc-w-h-angle_rad-target_size","title":"<code>make_model_canvas_and_meta(base_img, xc, yc, w, h, angle_rad, target_size)</code>","text":"<p>Extracts a rotated crop and prepares it for model inference:</p> <ol> <li> <p>Calculates rotated rectangle corners</p> </li> <li> <p>Creates polygon mask</p> </li> <li> <p>Centers crop on black canvas (maintains spatial context)</p> </li> <li> <p>Resizes to model input size (224\u00d7224)</p> </li> <li> <p>Applies ResNet50 preprocessing</p> </li> </ol> <p>Returns preprocessed array and metadata for reverse coordinate transformation.</p>"},{"location":"developer-guide/api-reference/#security-considerations","title":"Security Considerations","text":""},{"location":"developer-guide/api-reference/#cors-configuration","title":"CORS Configuration","text":"<p>Only whitelisted origins can access the API: <pre><code>ALLOWED_ORIGINS = [\n    \"http://localhost:4001\",\n    \"https://ai-roi-detector.biodata.di.unimi.it\"\n]\n</code></pre></p> <p>To add new origins, modify <code>ALLOWED_ORIGINS</code> in <code>api/server.py</code>.</p>"},{"location":"developer-guide/api-reference/#next-steps","title":"Next Steps","text":"<ul> <li>Architecture - System design details</li> <li>Installation - Setup guide</li> <li>Overview - Project introduction</li> </ul>"},{"location":"developer-guide/architecture/","title":"Architecture","text":"<p>Deep dive into the ROI Detector's technical implementation.</p>"},{"location":"developer-guide/architecture/#frontend-architecture","title":"Frontend Architecture","text":""},{"location":"developer-guide/architecture/#state-management","title":"State Management","text":"<p>The application maintains global state through JavaScript variables:</p> <ul> <li>Image data: <code>photoImageData</code>, <code>fluoImageData</code>, natural dimensions</li> <li>Crops: Array of crop objects with position, size, rotation, and predicted ROIs</li> <li>Selection: <code>activeIndex</code> for crops, <code>selectedROI</code> for individual ROIs</li> <li>View state: Current view mode (<code>photo</code>/<code>fluo</code>), luminance threshold</li> </ul>"},{"location":"developer-guide/architecture/#canvas-system","title":"Canvas System","text":"<p>Two-layer rendering:</p> <ol> <li> <p>Base layer (<code>&lt;img&gt;</code>) - Displays the photograph or fluorescence image</p> </li> <li> <p>Overlay layer (<code>&lt;canvas&gt;</code>) - Renders crops, ROIs, handles, and overlays</p> </li> </ol> <p>Coordinate transformations map between image space and canvas space: <pre><code>// Image \u2192 Canvas\nimgToCanvas(x, y) = (x * scale + xOff, y * scale + yOff)\n\n// Canvas \u2192 Image\ncanvasToImgPoint(x, y) = ((x - xOff) / scale, (y - yOff) / scale)\n</code></pre></p> <p>Coordinate System</p> <p>All coordinates are stored in image space (original pixel coordinates). Canvas transformations are applied only during rendering to account for viewport size and zoom level.</p>"},{"location":"developer-guide/architecture/#interaction-system","title":"Interaction System","text":"<p>Drag-and-drop system with modes:</p> <ul> <li> <p>Move - Drag crop border or ROI body</p> </li> <li> <p>Resize - Drag corner/edge handles</p> </li> <li> <p>Rotate - Drag rotation handle, Shift+Scroll, or double-click ROI + drag</p> </li> </ul> <p>Keyboard shortcuts: Arrow keys move selected ROI by 5px increments with acceleration.</p>"},{"location":"developer-guide/architecture/#backend-architecture","title":"Backend Architecture","text":""},{"location":"developer-guide/architecture/#api-structure","title":"API Structure","text":"<p>FastAPI application in <code>api/server.py</code>:</p> <ul> <li> <p>Prefix: <code>/api</code></p> </li> <li> <p>CORS: Enabled for whitelisted origins</p> </li> <li> <p>Documentation: Auto-generated at <code>/api/docs</code></p> </li> </ul>"},{"location":"developer-guide/architecture/#image-cache","title":"Image Cache","text":"<p>In-memory dictionary keyed by session ID: <pre><code>IMAGE_CACHE[f\"original_{session_id}\"] = PIL.Image  # Original TIFF\nIMAGE_CACHE[f\"preview_{session_id}\"] = PIL.Image   # 8-bit preview\n</code></pre></p> <p>Cache is cleared via <code>/housekeeping</code> endpoint.</p> <p>Memory Limitations</p> <p>Images are stored in RAM. For production with many concurrent users, consider using other solutions instead of in-memory caching.</p>"},{"location":"developer-guide/architecture/#processing-pipeline","title":"Processing Pipeline","text":""},{"location":"developer-guide/architecture/#tiff-normalization","title":"TIFF Normalization","text":"<p>16-bit TIFF \u2192 8-bit for display:</p> <ul> <li> <p>Standard images: Linear min-max scaling</p> </li> <li> <p>Fluorescence: Percentile-based (1st-99.5th) for better contrast</p> </li> </ul>"},{"location":"developer-guide/architecture/#crop-extraction","title":"Crop Extraction","text":"<p><code>make_model_canvas_and_meta()</code> function:</p> <ol> <li> <p>Calculate rotated corners with rotation matrix</p> </li> <li> <p>Create polygon mask for crop region</p> </li> <li> <p>Center crop on black canvas (maintains spatial context)</p> </li> <li> <p>Resize to 224\u00d7224</p> </li> <li> <p>Apply ResNet50 preprocessing</p> </li> </ol> <p>Returns preprocessed array + metadata for reverse coordinate transformation.</p>"},{"location":"developer-guide/architecture/#model-management","title":"Model Management","text":"<p>Models loaded on-demand and cached: <pre><code>MODEL_CACHE = {}  # {model_name: keras.Model}\n\nAVAILABLE_MODELS = {\n    \"run_20251208_105627\": {\"dual_input\": False},\n    \"run_20251219_171038\": {\"dual_input\": True}\n}\n</code></pre></p> <p>Model Loading</p> <p>Models are loaded once on first request and remain in memory for the lifetime of the server. Restart the server to reload updated models.</p>"},{"location":"developer-guide/architecture/#prediction-workflow","title":"Prediction Workflow","text":"<p><code>/predict_rois</code> endpoint:</p> <ol> <li> <p>Load images from cache</p> </li> <li> <p>Process each crop (extract, center, resize, preprocess)</p> </li> <li> <p>Prepare batch input (single or dual)</p> </li> <li> <p>Run inference: <code>model(x, training=False)</code></p> </li> <li> <p>Parse output (centers + angles)</p> </li> <li> <p>Transform coordinates back to original image space</p> </li> <li> <p>Return JSON with predictions</p> </li> </ol> <p>Coordinate transformation: <pre><code>Normalized [0,1] \u2192 Canvas coords \u2192 Adjust for offset \u2192 Absolute image coords\n</code></pre></p>"},{"location":"developer-guide/architecture/#machine-learning","title":"Machine Learning","text":""},{"location":"developer-guide/architecture/#model-architecture","title":"Model Architecture","text":"<p>Both models use: <pre><code>Input(s) \u2192 ResNet50 (frozen) \u2192 GlobalAvgPool \u2192 Dense \u2192 Dropout \u2192 Output Heads\n</code></pre></p> <p>Single-input model: Photo only (224\u00d7224\u00d73)  </p> <p>Dual-input model: Photo + Fluorescence (concatenated after ResNet50)</p> <p>Output heads: - Centers: Dense(14, sigmoid) \u2192 (x, y) for 7 ROIs - Angles: Dense(7, tanh) \u2192 rotation angles</p>"},{"location":"developer-guide/architecture/#fixed-roi-dimensions","title":"Fixed ROI Dimensions","text":"<p>ROI sizes are constant (normalized to image dimensions): <pre><code>ROI_W = [0.0929, 0.1251, 0.1928, 0.1939, 0.1195, 0.0907, 0.0330]\nROI_H = [0.1373, 0.1461, 0.1417, 0.1395, 0.1195, 0.0441, 0.0863]\n</code></pre></p> <p>Models predict only position (x, y) and orientation (angle).</p> <p>Design Choice</p> <p>Fixed ROI sizes simplify the model and improve generalization. Mouse anatomy is sufficiently consistent that learning position and orientation is sufficient for accurate ROI placement.</p>"},{"location":"developer-guide/architecture/#training-pipeline","title":"Training Pipeline","text":"<p>Managed by <code>run.py</code>:</p> <ol> <li><code>--tune</code> - KerasTuner searches learning rate, dropout, dense units</li> <li><code>--train</code> - Train dense layers (ResNet50 frozen)</li> <li><code>--finetune_last_n</code> - Unfreeze last N ResNet50 layers, train with low LR</li> <li><code>--finetune_all</code> - Unfreeze entire network (optional)</li> </ol> <p>Custom metrics: IoU and Coverage computed using Shapely for polygon intersections.</p>"},{"location":"developer-guide/architecture/#extensibility","title":"Extensibility","text":""},{"location":"developer-guide/architecture/#adding-models","title":"Adding Models","text":"<ol> <li>Save to <code>outputs/run_YYYYMMDD_HHMMSS/</code></li> <li>Add to <code>AVAILABLE_MODELS</code> dict</li> <li>Frontend auto-detects via <code>/models</code> endpoint</li> </ol> <p>Model Naming Convention</p> <p>Use the format <code>run_YYYYMMDD_HHMMSS</code> for consistency. The frontend expects models in <code>outputs/{model_name}/fine_tune_last_n/model_finetuned_last_n.keras</code>.</p>"},{"location":"developer-guide/architecture/#next-steps","title":"Next Steps","text":"<ul> <li>API Reference - Endpoint documentation</li> <li>Installation - Setup guide</li> </ul>"},{"location":"developer-guide/installation/","title":"Installation","text":"<p>This guide walks you through setting up the ROI Detector development environment.</p>"},{"location":"developer-guide/installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Operating System: Linux (tested on Ubuntu 24)</li> <li>Python: 3.11</li> <li>CUDA: 12.4 (for GPU acceleration)</li> <li>Conda: Miniconda or Anaconda</li> </ul>"},{"location":"developer-guide/installation/#environment-setup","title":"Environment Setup","text":""},{"location":"developer-guide/installation/#create-conda-environment","title":"Create Conda Environment","text":"<p>The project uses a Conda environment with all required dependencies: <pre><code>conda env create -f environment.yml\nconda activate ottobrini\n</code></pre></p> <p>This will install:</p> <ul> <li>FastAPI &amp; Uvicorn - Backend server</li> <li>TensorFlow 2.18 - ML framework with CUDA 12.4 support</li> <li>Pillow, NumPy, Shapely - Image and geometry processing</li> <li>scikit-learn, scipy - Statistical analysis</li> <li>Jupyter, Matplotlib - Development and visualization</li> </ul>"},{"location":"developer-guide/installation/#running-the-application","title":"Running the Application","text":""},{"location":"developer-guide/installation/#start-the-backend-server","title":"Start the Backend Server","text":"<pre><code>uvicorn api.server:app --port 4002 --reload --host 0.0.0.0\n</code></pre>"},{"location":"developer-guide/installation/#start-the-frontend","title":"Start the Frontend","text":"<p>For development, use a simple HTTP server: <pre><code># From Src/single_output directory\npython -m http.server 4001\n</code></pre></p>"},{"location":"developer-guide/installation/#next-steps","title":"Next Steps","text":"<ul> <li>Architecture Details - Understand system components</li> <li>API Reference - Explore available endpoints</li> </ul>"},{"location":"developer-guide/overview/","title":"Developer Guide Overview","text":"<p>The ROI Detector is a full-stack machine learning application for predicting and analyzing Regions of Interest (ROIs) on fluorescence microscopy images of lab mice.</p>"},{"location":"developer-guide/overview/#architecture-overview","title":"Architecture Overview","text":"<p>The ROI Detector is built on a three-tier architecture that separates concerns between frontend, backend and machine learning inference.</p>"},{"location":"developer-guide/overview/#frontend-layer","title":"Frontend Layer","text":"<p>The browser-based interface handles all user interactions and visual feedback. Built with JavaScript and HTML, it manages:</p> <ul> <li>Image display and manipulation - Renders uploaded images with zoom and pan capabilities</li> <li>Interactive crop definition - Users draw and adjust bounding boxes around each mouse</li> <li>ROI visualization - Displays predicted regions with color-coded overlays</li> <li>Real-time editing - Supports drag-to-move, rotation, and keyboard-based adjustments</li> <li>Statistical overlays - Shows luminance thresholds and significance testing results</li> </ul> <p>The frontend communicates with the backend exclusively through REST API calls, maintaining a stateless architecture.</p>"},{"location":"developer-guide/overview/#backend-layer","title":"Backend Layer","text":"<p>A FastAPI server provides the computational backbone of the application:</p> <ul> <li>Image preprocessing - Converts TIFF files to 8-bit format, applies normalization, and handles rotation-invariant cropping</li> <li>Model orchestration - Loads and manages multiple trained models, routing requests based on user selection</li> <li>Coordinate transformations - Maps between canvas coordinates, image space, and normalized model inputs</li> <li>Session management - Maintains an in-memory cache of uploaded images using session IDs</li> <li>Export formatting - Generates output files compatible with the original imaging system</li> </ul> <p>The backend is stateless except for the image cache, which is periodically cleared via housekeeping endpoints.</p>"},{"location":"developer-guide/overview/#machine-learning-layer","title":"Machine Learning Layer","text":"<p>Two ResNet50-based models provide ROI predictions:</p> <ul> <li>Single-input model - Trained on photograph data alone, suitable for basic ROI detection</li> <li>Dual-input model - Accepts both photograph and luminescence images, providing enhanced accuracy by leveraging complementary information</li> </ul> <p>Both models output normalized coordinates (x, y, angle) for 7 anatomical ROIs per mouse. The backend denormalizes these predictions back to pixel coordinates before returning them to the frontend.</p>"},{"location":"developer-guide/overview/#request-response-flow","title":"Request-Response Flow","text":"<p>A typical user session follows this pattern:</p> <ol> <li>Image upload - User drops a TIFF file onto the interface</li> <li>Preview generation - Backend converts the image to PNG and returns a data URL with a session ID</li> <li>Crop definition - User draws bounding boxes around each mouse in the image</li> <li>Prediction request - Frontend sends crop coordinates to <code>/predict_rois</code> endpoint</li> <li>Model inference - Backend extracts each crop, preprocesses it, and runs inference</li> <li>Coordinate transformation - Predictions are mapped from model space back to original image coordinates</li> <li>Visualization - Frontend renders ROIs as colored rectangles and circles on the canvas</li> <li>Export - User clicks export, triggering client-side generation of <code>AnalyzedClickInfo.txt</code></li> </ol>"},{"location":"developer-guide/overview/#technology-stack","title":"Technology Stack","text":""},{"location":"developer-guide/overview/#frontend","title":"Frontend","text":"<ul> <li>HTML5 - Application structure</li> <li>JavaScript - UI logic and interactions</li> <li>Canvas API - Image rendering and ROI visualization</li> <li>Bootstrap 5 - UI components and styling</li> </ul>"},{"location":"developer-guide/overview/#backend","title":"Backend","text":"<ul> <li>FastAPI - RESTful API framework</li> <li>Python 3.11 - Core language</li> <li>Pillow (PIL) - Image processing</li> <li>NumPy - Numerical computations</li> <li>TensorFlow/Keras - Model inference</li> </ul>"},{"location":"developer-guide/overview/#machine-learning","title":"Machine Learning","text":"<ul> <li>ResNet50 - Base architecture (transfer learning)</li> <li>Custom Dense Layers - ROI coordinate regression</li> <li>TensorFlow 2.18 - Training and inference framework</li> </ul>"},{"location":"developer-guide/overview/#key-features","title":"Key Features","text":""},{"location":"developer-guide/overview/#interactive-editing","title":"Interactive Editing","text":"<ul> <li>Drag-and-drop crop creation and adjustment</li> <li>ROI selection, movement, and rotation</li> <li>Constraint-based movement (ROIs stay within parent crop)</li> <li>Keyboard shortcuts for precise control</li> </ul>"},{"location":"developer-guide/overview/#statistical-analysis","title":"Statistical Analysis","text":"<ul> <li>Luminance threshold visualization</li> <li>Paired t-test for signal significance (fluorescence vs. photo)</li> <li>ROI projection on luminance profile chart</li> <li>User evaluation logging with IP tracking</li> </ul>"},{"location":"developer-guide/overview/#exportimport","title":"Export/Import","text":"<ul> <li>Export ROIs to <code>AnalyzedClickInfo.txt</code> format (compatible with imaging system)</li> <li>Import existing ROIs for editing</li> <li>Optional metadata file merging</li> </ul>"},{"location":"developer-guide/overview/#next-steps","title":"Next Steps","text":"<ul> <li>Installation Guide - Set up the development environment</li> <li>Architecture Details - Deep dive into system components</li> <li>API Reference - Complete API documentation</li> </ul>"},{"location":"user-guide/example/","title":"Example","text":"<p>A step-by-step walkthrough of a typical workflow with screenshots.</p>"},{"location":"user-guide/example/#1-load-the-photo-image","title":"1. Load the Photo Image","text":"<p>Open the application and click Load Photo Image to select your TIFF file.</p> <p></p>"},{"location":"user-guide/example/#2-load-luminescence-optional","title":"2. Load Luminescence (Optional)","text":"<p>For dual-input analysis, enable the Luminescence model switch and load the fluorescence image.</p> <p></p> <p>Flexible Loading</p> <p>The fluorescence image can be loaded at any time:</p> <ul> <li>Before processing - Load it now to use the dual-input model immediately</li> <li>After prediction - Load it later if you initially used the anatomy-only model and want to analyze signal intensity</li> </ul>"},{"location":"user-guide/example/#3-add-crops","title":"3. Add Crops","text":"<p>Click Add Crop to create a bounding box around each mouse.</p> <p></p> <p>Multiple Mice</p> <p>The same procedure works for images containing multiple mice. Simply add one crop per mouse (e.g., 3 crops for 3 mice). Each crop will be processed independently.</p>"},{"location":"user-guide/example/#4-adjust-the-crop","title":"4. Adjust the Crop","text":"<p>Resize and position the crop to fit the mouse. Use the corner handles to resize and the circular handle to rotate.</p> <p></p>"},{"location":"user-guide/example/#5-process-rois","title":"5. Process ROIs","text":"<p>Click Process ROIs to run the prediction model.</p> <p></p>"},{"location":"user-guide/example/#6-review-predictions","title":"6. Review Predictions","text":"<p>The model outputs 7 ROIs per mouse. Click on any ROI to select and adjust it.</p> <p></p>"},{"location":"user-guide/example/#7-place-control-roi-optional","title":"7. Place Control ROI (Optional)","text":"<p>Double-click on a dark area of the image to place a Control ROI for background light measurement.</p> <p>Control ROI</p> <p>The Control ROI is used to check baseline fluorescence levels. It appears as a white circle and is automatically assigned:</p> <ul> <li>ID 8 for single-mouse images</li> <li>ID 15 for multi-mouse images</li> </ul>"},{"location":"user-guide/example/#8-analyze-signal","title":"8. Analyze Signal","text":"<p>Switch to luminescence view and adjust the threshold slider to highlight relevant pixels.</p> <p></p>"},{"location":"user-guide/example/#9-export-results","title":"9. Export Results","text":"<p>Click Export ROIs to download the results in a format compatible with the imaging system.</p> <p></p> <p>The exported file is named <code>AnalyzedClickInfo.txt</code> and contains all ROI coordinates in the original TIFF format.</p>"},{"location":"user-guide/example/#example-output","title":"Example Output","text":"<p>Here's a sample of the exported <code>AnalyzedClickInfo.txt</code> file: <pre><code>*** photographic image: photograph.TIF\nAcquisition Date: Monday, December 29, 2025\nAcquisition Time: 14:32:15\nBinning Factor: 4\n\n*** ROIs: 7 ROIs for this Click\nROI 1: zColorIndex=1;ROI Type=Measurement;ColorTable=BlueRedGreen;Subject ROI=_none_;Subject ID=;Subject Label=;Bkg ROI=_none_;LineSize=2.000000;Locked=0;PositionLocked=0;Shape=Square;Xc=955.071720546938;Yc=1254.6621;Width=178.3680;Height=263.7024;Angle=5.2341;Label=ROI 1;cm per pixel=0.006979166667;ROIColor=255;\nROI 2: zColorIndex=1;ROI Type=Measurement;ColorTable=BlueRedGreen;Subject ROI=_none_;Subject ID=;Subject Label=;Bkg ROI=_none_;LineSize=2.000000;Locked=0;PositionLocked=0;Shape=Square;Xc=985.234567890123;Yc=1389.4532;Width=240.1920;Height=280.5312;Angle=5.1987;Label=ROI 2;cm per pixel=0.006979166667;ROIColor=255;\nROI 3: zColorIndex=1;ROI Type=Measurement;ColorTable=BlueRedGreen;Subject ROI=_none_;Subject ID=;Subject Label=;Bkg ROI=_none_;LineSize=2.000000;Locked=0;PositionLocked=0;Shape=Square;Xc=1012.567890123456;Yc=1542.8765;Width=370.1760;Height=272.0544;Angle=5.3124;Label=ROI 3;cm per pixel=0.006979166667;ROIColor=255;\nROI 4: zColorIndex=1;ROI Type=Measurement;ColorTable=BlueRedGreen;Subject ROI=_none_;Subject ID=;Subject Label=;Bkg ROI=_none_;LineSize=2.000000;Locked=0;PositionLocked=0;Shape=Square;Xc=1045.678901234567;Yc=1698.2341;Width=372.2880;Height=267.8400;Angle=5.2876;Label=ROI 4;cm per pixel=0.006979166667;ROIColor=255;\nROI 5: zColorIndex=1;ROI Type=Measurement;ColorTable=BlueRedGreen;Subject ROI=_none_;Subject ID=;Subject Label=;Bkg ROI=_none_;LineSize=2.000000;Locked=0;PositionLocked=0;Shape=Circle;Xc=931.600940981105;Yc=1453.0745;Width=229.4400;Height=229.4400;Angle=0.0000;Label=ROI 5;cm per pixel=0.006979166667;ROIColor=255;\nROI 6: zColorIndex=1;ROI Type=Measurement;ColorTable=BlueRedGreen;Subject ROI=_none_;Subject ID=;Subject Label=;Bkg ROI=_none_;LineSize=2.000000;Locked=0;PositionLocked=0;Shape=Square;Xc=1078.345678901234;Yc=1845.3456;Width=174.1440;Height=84.6720;Angle=5.4231;Label=ROI 6;cm per pixel=0.006979166667;ROIColor=255;\nROI 7: zColorIndex=1;ROI Type=Measurement;ColorTable=BlueRedGreen;Subject ROI=_none_;Subject ID=;Subject Label=;Bkg ROI=_none_;LineSize=2.000000;Locked=0;PositionLocked=0;Shape=Square;Xc=1095.123456789012;Yc=1978.5432;Width=63.3600;Height=165.6960;Angle=5.3987;Label=ROI 7;cm per pixel=0.006979166667;ROIColor=255;\n</code></pre></p> <p>Output Format</p> <ul> <li>Coordinates are scaled to the original TIFF dimensions (1920px)</li> <li>If metadata was loaded, it will be preserved and merged with the new ROIs</li> </ul>"},{"location":"user-guide/work-instructions/","title":"Work instructions","text":"<p>This guide explains how to use the ROI Predictor application to predict and manage Regions of Interest on lab mice images.</p>"},{"location":"user-guide/work-instructions/#getting-started","title":"Getting Started","text":""},{"location":"user-guide/work-instructions/#loading-an-image","title":"Loading an Image","text":"<ol> <li>Click Load Photo Image to select a photograph (supports TIFF and common image formats)</li> <li>The image will appear in the central canvas area</li> <li>You can also drag and drop an image directly onto the canvas</li> </ol> <p>Supported Formats</p> <p>The application works best with <code>.tif</code> / <code>.tiff</code> files from the imaging system. Standard formats like PNG and JPEG are also supported.</p>"},{"location":"user-guide/work-instructions/#selecting-a-model","title":"Selecting a Model","text":"<p>Use the ML model switch at the top of the sidebar to choose the prediction model:</p> Mode Description Off (default) Single-input model \u2014 uses only the photograph On (Luminescence) Dual-input model \u2014 requires both photograph and luminescence image <p>Dual-Input Model</p> <p>When the dual-input model is selected, the Load Luminescence button becomes available. You must load the luminescence image before processing.</p>"},{"location":"user-guide/work-instructions/#working-with-crops","title":"Working with Crops","text":"<p>Crops define the regions containing individual mice that will be processed by the model.</p>"},{"location":"user-guide/work-instructions/#adding-a-crop","title":"Adding a Crop","text":"<ol> <li>Click Add Crop to create a new bounding box</li> <li>The crop appears centered on the image</li> <li>Each crop is assigned a unique color for easy identification</li> </ol>"},{"location":"user-guide/work-instructions/#adjusting-a-crop","title":"Adjusting a Crop","text":"Action How to Move Click and drag the crop border Resize Drag the corner or edge handles Rotate Drag the circular handle above the crop, or hold Shift and scroll <p>Multiple Crops</p> <p>You can add multiple crops for images containing more than one mouse. Each crop is processed independently.</p>"},{"location":"user-guide/work-instructions/#removing-crops","title":"Removing Crops","text":"<ul> <li>Click the \u00d7 button on a crop's top-right corner to remove it individually</li> <li>Click the \ud83d\uddd1\ufe0f button (bottom-right) to clear all crops at once</li> </ul>"},{"location":"user-guide/work-instructions/#predicting-rois","title":"Predicting ROIs","text":"<p>Once you have defined the crops:</p> <ol> <li>Click Process ROIs</li> <li>The model predicts 7 ROIs per crop</li> </ol>"},{"location":"user-guide/work-instructions/#roi-mapping","title":"ROI Mapping","text":"ROI Anatomical Region Shape 1 Head Rectangle 2 Upper body Rectangle 3 Mid body Rectangle 4 Lower body Rectangle 5 Genital area Circle 6 Hind limbs Rectangle 7 Tail Rectangle <p>Processing Requirements</p> <p>For the dual-input model, ensure both photograph and luminescence images are loaded before clicking Process ROIs.</p>"},{"location":"user-guide/work-instructions/#editing-rois","title":"Editing ROIs","text":""},{"location":"user-guide/work-instructions/#selecting-an-roi","title":"Selecting an ROI","text":"<p>Click on any ROI to select it. When selected:</p> <ul> <li>A yellow dashed border highlights the ROI</li> <li>The control panel appears below the canvas</li> <li>The ROI ID is displayed in the panel header</li> </ul>"},{"location":"user-guide/work-instructions/#movement-and-rotation","title":"Movement and Rotation","text":"Action Method Move Single click + drag, or use control panel arrows, or Up Down Left Right keys Rotate Double click + drag (rectangles only) <p>Constraints</p> <p>ROIs are constrained within their parent crop boundaries. Circular ROIs (ROI 5) cannot be rotated.</p>"},{"location":"user-guide/work-instructions/#luminescence-view","title":"Luminescence View","text":"<p>When working with the dual-input model, you can analyze fluorescence signals.</p>"},{"location":"user-guide/work-instructions/#switching-views","title":"Switching Views","text":"<p>Use the image switch below the canvas to toggle between:</p> <ul> <li>Photo view \u2014 standard photograph</li> <li>Luminescence view \u2014 fluorescence image with analysis tools</li> </ul>"},{"location":"user-guide/work-instructions/#analysis-tools","title":"Analysis Tools","text":"<p>In luminescence view, additional tools become available:</p>"},{"location":"user-guide/work-instructions/#threshold-slider","title":"Threshold Slider","text":"<p>A vertical slider appears on the left side of the canvas:</p> <ul> <li>Adjust the luminance threshold (0-100%)</li> <li>Pixels above the threshold are highlighted within ROIs</li> <li>Highlighted pixels use the crop's color with transparency</li> </ul>"},{"location":"user-guide/work-instructions/#luminance-profile-chart","title":"Luminance Profile Chart","text":"<p>A chart appears below the canvas showing:</p> <ul> <li>Intensity profile along the crop's long axis</li> <li>Yellow dashed line indicating the current threshold</li> <li>Yellow shaded area showing the selected ROI projection</li> </ul> <p>Interpreting the Chart</p> <p>Peaks in the luminance profile indicate areas of high fluorescence signal. Use this to identify relevant signal regions.</p>"},{"location":"user-guide/work-instructions/#control-roi","title":"Control ROI","text":"<p>The Control ROI is a reference region used to check background light levels.</p> <p>To place a Control ROI:</p> <ol> <li>Double-click anywhere on the image</li> <li>Confirm placement in the popup dialog</li> <li>The Control ROI appears as a white circle</li> </ol> <p>Control ROI ID</p> <p>The Control ROI is automatically assigned ID 8 for single-mouse images, or ID 15 for multi-mouse images.</p>"},{"location":"user-guide/work-instructions/#evaluating-signal-relevance","title":"Evaluating Signal Relevance","text":"<p>In luminescence view, you can evaluate whether the fluorescence signal in an ROI is biologically relevant:</p> <ol> <li>Right-click on an ROI that contains illuminated pixels</li> <li>A popup asks: \"Is ROI X signal relevant?\"</li> <li>Click YES or NO to record your evaluation</li> </ol> <p>Statistical Analysis</p> <p>When an ROI is selected, the p-value from a paired t-test (fluorescence vs. photograph) is displayed next to the ROI.</p>"},{"location":"user-guide/work-instructions/#importing-and-exporting","title":"Importing and Exporting","text":""},{"location":"user-guide/work-instructions/#loading-metadata","title":"Loading Metadata","text":"<p>Optionally load an existing metadata file:</p> <ol> <li>Click Load Metadata (Optional)</li> <li>Select an <code>AnalyzedClickInfo.txt</code> file</li> <li>The filename appears in the status indicator</li> </ol> <p>Metadata Merge</p> <p>When exporting, ROIs will be merged into the loaded metadata file, preserving other information.</p>"},{"location":"user-guide/work-instructions/#importing-rois","title":"Importing ROIs","text":"<p>Load previously saved ROI coordinates:</p> <ol> <li>Click Import ROIs</li> <li>Select a text file containing ROI data</li> </ol> Supported Import Formats <ul> <li>Standard <code>AnalyzedClickInfo.txt</code> format with <code>ROI N: Xc=...; Yc=...;</code> syntax</li> <li>Comma or space-separated coordinates</li> <li>Key-value format: <code>xc=0.5 yc=0.3 angle=0</code></li> </ul>"},{"location":"user-guide/work-instructions/#exporting-rois","title":"Exporting ROIs","text":"<p>Save predicted or edited ROIs:</p> <ol> <li>After processing, click Export ROIs</li> <li>A text file is downloaded automatically</li> </ol> <p>The exported file contains:</p> <ul> <li>ROI coordinates scaled to original TIFF dimensions (1920px)</li> <li>Format compatible with the original imaging system</li> <li>All metadata from the loaded file (if any)</li> </ul> <p>Overwrite Warning</p> <p>If the loaded metadata already contains ROIs, you will be asked to confirm before overwriting.</p>"},{"location":"user-guide/work-instructions/#keyboard-shortcuts","title":"Keyboard Shortcuts","text":"Shortcut Action Up Down Left Right Move selected ROI Shift + Scroll Rotate active crop Double-click (on ROI) Enter rotation mode Double-click (on canvas) Place Control ROI Right-click (on ROI) Evaluate signal (luminescence view)"},{"location":"user-guide/work-instructions/#tips-best-practices","title":"Tips &amp; Best Practices","text":"<p>Workflow Recommendations</p> <ol> <li>Always load the photograph first, then the luminescence image</li> <li>The luminescence image is automatically resized to match photograph dimensions</li> <li>Adjust the threshold slider to filter background noise</li> <li>Use the luminance chart to identify signal peaks along the mouse body</li> <li>Place a Control ROI in a dark area to establish baseline levels</li> </ol>"}]}